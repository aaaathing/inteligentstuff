{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333fafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e419e1b",
   "metadata": {},
   "source": [
    "# attempt 1\n",
    "Sep 12, 2025\n",
    "\n",
    "chatgpt wrote most of it  \n",
    "but it is too complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b0039",
   "metadata": {},
   "source": [
    "can a cnn have attention where each layer's attention depends on the next layer\n",
    "\n",
    "ChatGPT said:\n",
    "\n",
    "In standard convolutional neural networks (CNNs), attention is not inherently built-in like in transformers. However, yes, it is possible to design a CNN with attention mechanisms, and you can engineer the attention in such a way that each layer's attention depends on the next layer—but this is non-standard and requires a custom architecture.\n",
    "\n",
    "Let’s break this down:\n",
    "\n",
    "🔁 Traditional Attention in CNNs (Top-down or Bottom-up?)\n",
    "\n",
    "Most attention mechanisms in CNNs (e.g., Squeeze-and-Excitation, CBAM, Self-Attention) work within a layer or use bottom-up information flow — that is, attention is computed from the current or previous layer, not from future layers.\n",
    "\n",
    "✅ Can CNNs Have Attention That Depends on Future Layers?\n",
    "\n",
    "Yes, in theory. But you need to:\n",
    "\n",
    "Invert the standard feedforward flow (to make attention depend on future layers),\n",
    "\n",
    "Or design a recurrent / iterative refinement mechanism (e.g., feedback loops),\n",
    "\n",
    "Or use multi-pass processing (e.g., compute the later layers, then update earlier ones with attention informed by the later layers).\n",
    "\n",
    "🔧 Example Approaches:\n",
    "1. Top-Down Feedback Attention\n",
    "\n",
    "A mechanism where later layers send feedback to earlier layers — like in some biologically inspired CNNs or recurrent attention networks. This introduces attention that depends on the next layer.\n",
    "\n",
    "2. Iterative Refinement Networks\n",
    "\n",
    "CNN passes data forward.\n",
    "\n",
    "Attention maps are computed at higher layers.\n",
    "\n",
    "Then a second pass is done where earlier layers are refined using that high-level attention.\n",
    "\n",
    "See: RefineNet, Stacked Hourglass Networks, or U-Net with Attention Feedback.\n",
    "\n",
    "3. Bidirectional Networks\n",
    "\n",
    "Like Bi-RNNs, where forward and backward passes are computed — in a CNN context, this is rare but has been explored for segmentation and attention.\n",
    "\n",
    "4. Cross-Layer Attention\n",
    "\n",
    "Design attention modules that allow layers to attend to outputs of later layers — similar to how transformers can attend across layers. You need to compute the later layers first, or process in parallel with cross-layer connections.\n",
    "\n",
    "⚠️ Caveats:\n",
    "\n",
    "You break the feedforward paradigm — either via recurrence, delay, or architectural complexity.\n",
    "\n",
    "In practice, such dependencies are harder to train and may be less efficient, especially on GPUs optimized for sequential layers.\n",
    "\n",
    "Requires careful handling of gradient flow and memory usage.\n",
    "\n",
    "✅ Summary:\n",
    "Question\tAnswer\n",
    "Can CNNs have attention?\tYes\n",
    "Can that attention depend on next layer?\tYes, with custom design\n",
    "Is it standard?\tNo, requires feedback/recurrence/multi-pass\n",
    "\n",
    "If you're building something specific (e.g., segmentation, recognition), I can help you sketch an architecture that supports this top-down attention. Would you like an example or diagram?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3ee398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseBlock(nn.Module):\n",
    "    def __init__(self, block_sequence):\n",
    "        super().__init__()\n",
    "        self.original_blocks = block_sequence\n",
    "        self.deconv_blocks = nn.ModuleList()\n",
    "\n",
    "        for block in reversed(block_sequence):\n",
    "            assert isinstance(block, models.resnet.BasicBlock), \"Only BasicBlock is supported\"\n",
    "\n",
    "            conv2 = block.conv2\n",
    "            bn2 = block.bn2\n",
    "            conv1 = block.conv1\n",
    "            bn1 = block.bn1\n",
    "\n",
    "            # Deconv layers reverse the order of convs in forward pass\n",
    "            deconv_block = nn.Sequential(\n",
    "                bn2,\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=conv2.out_channels,\n",
    "                    out_channels=conv2.in_channels,\n",
    "                    kernel_size=conv2.kernel_size,\n",
    "                    stride=conv2.stride,\n",
    "                    padding=conv2.padding,\n",
    "                    output_padding=1 if conv2.stride == (2, 2) else 0,\n",
    "                    bias=False\n",
    "                ),\n",
    "                #nn.BatchNorm2d(conv1.out_channels),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                bn1,\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=conv1.out_channels,\n",
    "                    out_channels=conv1.in_channels,\n",
    "                    kernel_size=conv1.kernel_size,\n",
    "                    stride=conv1.stride,\n",
    "                    padding=conv1.padding,\n",
    "                    output_padding=1 if conv1.stride == (2, 2) else 0,\n",
    "                    bias=False\n",
    "                ),\n",
    "                #nn.BatchNorm2d(conv1.in_channels),\n",
    "                #nn.ReLU(inplace=True),\n",
    "            )\n",
    "            self.deconv_blocks.append(deconv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.deconv_blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    def sync_weights(self):\n",
    "        for block, deconv_block in zip(reversed(self.original_blocks), self.deconv_blocks):\n",
    "            conv2 = block.conv2\n",
    "            conv1 = block.conv1\n",
    "\n",
    "            deconv1 = deconv_block[1]  # ConvTranspose2d (for conv2)\n",
    "            deconv2 = deconv_block[3]  # ConvTranspose2d (for conv1)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Transpose weights (out_channels, in_channels, h, w) -> (in, out, h, w)\n",
    "                deconv1.weight.copy_(conv2.weight)\n",
    "                deconv2.weight.copy_(conv1.weight)\n",
    "\n",
    "class FeedbackAttentionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        self.layer1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer2 = resnet.layer1\n",
    "        self.layer3 = resnet.layer2\n",
    "        self.layer4 = resnet.layer3\n",
    "        self.layer5 = resnet.layer4\n",
    "\n",
    "        # Reverse attention block that mirrors layer5\n",
    "        self.reverse_layer5 = ReverseBlock(self.layer5)\n",
    "        \n",
    "        # Classifier\n",
    "        self.avgpool = resnet.avgpool\n",
    "        self.fc = resnet.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "\n",
    "        # Sync weights from layer5 to reverse_layer5\n",
    "        self.reverse_layer5.sync_weights()\n",
    "\n",
    "        # Reverse attention: use x5 to attend to x2\n",
    "        attention = self.reverse_layer5(x5)  # approximate \"inverse\" of layer5\n",
    "\n",
    "        # Resize to match x2\n",
    "        #attention_resized = F.interpolate(attention, size=x2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        attention_map = torch.sigmoid(attention)\n",
    "        print(attention_map)\n",
    "\n",
    "        # Apply attention to earlier features\n",
    "        x4_attended = x4 * attention_map\n",
    "\n",
    "        # Re-run downstream layers\n",
    "        #x3_refined = self.layer3(x2_attended)\n",
    "        #x4_refined = self.layer4(x3_refined)\n",
    "        x5_refined = self.layer5(x4_attended)\n",
    "\n",
    "        out = self.avgpool(x5_refined)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7024a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# ImageNet normalization stats\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(\n",
    "    #    mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "    #    std=[0.229, 0.224, 0.225]    # ImageNet stds\n",
    "    #)\n",
    "])\n",
    "\n",
    "# Load image\n",
    "img_path = Path.home()/'Downloads/Screenshot 2025-04-10 10.50.49 AM.jpg'  # <-- Replace this\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "input_tensor = transform(image).unsqueeze(0)  # Shape: [1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5500e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "labels = urllib.request.urlopen(url).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bf88aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5416, 0.5799, 0.5379,  ..., 0.6320, 0.5666, 0.5414],\n",
      "          [0.5674, 0.6662, 0.5511,  ..., 0.7213, 0.5980, 0.6152],\n",
      "          [0.5085, 0.5292, 0.5039,  ..., 0.6425, 0.5751, 0.5546],\n",
      "          ...,\n",
      "          [0.5119, 0.5363, 0.4797,  ..., 0.5618, 0.5249, 0.5565],\n",
      "          [0.5051, 0.5325, 0.5170,  ..., 0.5629, 0.5263, 0.5068],\n",
      "          [0.4692, 0.5207, 0.4727,  ..., 0.5374, 0.4645, 0.5189]],\n",
      "\n",
      "         [[0.4885, 0.5095, 0.4832,  ..., 0.5347, 0.4992, 0.5427],\n",
      "          [0.4115, 0.4168, 0.4171,  ..., 0.4532, 0.4481, 0.4947],\n",
      "          [0.4536, 0.4289, 0.4270,  ..., 0.4776, 0.4788, 0.5090],\n",
      "          ...,\n",
      "          [0.4574, 0.4212, 0.4418,  ..., 0.3260, 0.4398, 0.4348],\n",
      "          [0.4758, 0.4198, 0.4858,  ..., 0.4264, 0.4557, 0.4742],\n",
      "          [0.4798, 0.4770, 0.4636,  ..., 0.4253, 0.4829, 0.4800]],\n",
      "\n",
      "         [[0.5096, 0.5371, 0.4917,  ..., 0.5414, 0.5122, 0.5292],\n",
      "          [0.5299, 0.5729, 0.4682,  ..., 0.5389, 0.5163, 0.5634],\n",
      "          [0.5091, 0.5393, 0.4953,  ..., 0.5408, 0.5173, 0.5547],\n",
      "          ...,\n",
      "          [0.6170, 0.7402, 0.6034,  ..., 0.7021, 0.5986, 0.6243],\n",
      "          [0.5188, 0.6229, 0.5210,  ..., 0.5885, 0.5197, 0.5478],\n",
      "          [0.5279, 0.6094, 0.5268,  ..., 0.5893, 0.5238, 0.5623]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5216, 0.4652, 0.5199,  ..., 0.5351, 0.5362, 0.4418],\n",
      "          [0.4695, 0.4023, 0.4508,  ..., 0.5603, 0.5087, 0.4855],\n",
      "          [0.4936, 0.4662, 0.4652,  ..., 0.5383, 0.5014, 0.4710],\n",
      "          ...,\n",
      "          [0.5331, 0.5078, 0.5330,  ..., 0.6400, 0.5929, 0.5460],\n",
      "          [0.5346, 0.5155, 0.5122,  ..., 0.5783, 0.5524, 0.4891],\n",
      "          [0.5143, 0.5030, 0.5148,  ..., 0.5635, 0.5645, 0.5096]],\n",
      "\n",
      "         [[0.4930, 0.5349, 0.5280,  ..., 0.5551, 0.5160, 0.5437],\n",
      "          [0.4865, 0.5487, 0.4843,  ..., 0.6113, 0.4749, 0.5311],\n",
      "          [0.4980, 0.5316, 0.5225,  ..., 0.5658, 0.5218, 0.5436],\n",
      "          ...,\n",
      "          [0.4927, 0.5674, 0.4888,  ..., 0.5997, 0.4657, 0.5622],\n",
      "          [0.4860, 0.5569, 0.4836,  ..., 0.5427, 0.4949, 0.5374],\n",
      "          [0.4526, 0.4689, 0.4156,  ..., 0.5040, 0.4624, 0.5133]],\n",
      "\n",
      "         [[0.5296, 0.4951, 0.4888,  ..., 0.4292, 0.5112, 0.4612],\n",
      "          [0.5135, 0.5250, 0.4290,  ..., 0.4497, 0.4975, 0.4817],\n",
      "          [0.4920, 0.4678, 0.4249,  ..., 0.4324, 0.4841, 0.4616],\n",
      "          ...,\n",
      "          [0.5137, 0.4706, 0.4414,  ..., 0.4794, 0.5187, 0.5103],\n",
      "          [0.5609, 0.6048, 0.5431,  ..., 0.5849, 0.5495, 0.5469],\n",
      "          [0.5257, 0.5465, 0.5178,  ..., 0.5425, 0.5271, 0.5334]]]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "b'bucket\\n'\n",
      "b'hook\\n'\n",
      "b'plunger\\n'\n",
      "b'ashcan\\n'\n",
      "b'water jug\\n'\n",
      "b'ladle\\n'\n",
      "b'water bottle\\n'\n",
      "b'pole\\n'\n",
      "b'paper towel\\n'\n",
      "b'cowboy hat\\n'\n"
     ]
    }
   ],
   "source": [
    "model = FeedbackAttentionCNN()\n",
    "output = model(input_tensor)\n",
    "for c in output.topk(10).indices[0]:\n",
    "\tprint(labels[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14fc08",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# attempt 2\n",
    "Sep 13, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ebc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "def getImage(path):\n",
    "\ttransform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True) ])\n",
    "\treturn transform(Image.open(Path.home()/path).convert('RGB')).unsqueeze(0)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "labels = urllib.request.urlopen(url).readlines()\n",
    "\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "input = getImage(\"Downloads/Screenshot 2025-04-10 10.50.49 AM.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ab5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad.zero_()\n",
    "input.requires_grad_(True)\n",
    "output = resnet(input)\n",
    "for c in output.topk(10).indices[0]:\n",
    "\tprint(labels[c])\n",
    "\n",
    "attentionToWhat = torch.zeros(output.shape)\n",
    "attentionToWhat[0,labels.index(b\"bucket\\n\")] = 1.0\n",
    "(output * attentionToWhat * output / 2.0).sum(1).backward()\n",
    "plt.imshow(input.grad[0].permute(1,2,0)*100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor(input, requires_grad=True)\n",
    "xinput = x\n",
    "\n",
    "x = resnet.conv1(x)\n",
    "x = resnet.bn1(x)\n",
    "x = resnet.relu(x)\n",
    "x = resnet.maxpool(x)\n",
    "x0 = x\n",
    "with torch.no_grad(): x0c = x = x.clone().requires_grad_()\n",
    "\n",
    "x = resnet.layer1(x)\n",
    "x1 = x\n",
    "with torch.no_grad(): x1c = x = x.clone().requires_grad_()\n",
    "x = resnet.layer2(x)\n",
    "x2 = x\n",
    "with torch.no_grad(): x2c = x = x.clone().requires_grad_()\n",
    "x = resnet.layer3(x)\n",
    "x3 = x\n",
    "with torch.no_grad(): x3c = x = x.clone().requires_grad_()\n",
    "x = resnet.layer4(x)\n",
    "x4 = x\n",
    "with torch.no_grad(): x4c = x = x.clone().requires_grad_()\n",
    "\n",
    "x = resnet.avgpool(x)\n",
    "x = torch.flatten(x, 1)\n",
    "x = resnet.fc(x)\n",
    "\n",
    "def rescale(x):\n",
    "\twith torch.no_grad():\n",
    "\t\tmin = x.min()\n",
    "\t\treturn (x - min) / (x.max() - min)\n",
    "\n",
    "attentionToWhat = torch.zeros(x.shape)\n",
    "attentionToWhat[0,labels.index(b\"bucket\\n\")] = 1.0\n",
    "(x*x/2.0 * attentionToWhat).sum().backward()\n",
    "\n",
    "(x4*x4/2.0 * rescale(x4c.grad)).sum().backward()\n",
    "(x3*x3/2.0 * rescale(x3c.grad)).sum().backward()\n",
    "(x2*x2/2.0 * rescale(x2c.grad)).sum().backward()\n",
    "(x1*x1/2.0 * rescale(x1c.grad)).sum().backward()\n",
    "(x0*x0/2.0 * rescale(x0c.grad)).sum().backward(retain_graph=True)\n",
    "\n",
    "def plottop(x, topcount=10):\n",
    "\tfig, axs = plt.subplots(1,topcount)\n",
    "\taidx = 0\n",
    "\tfor i in torch.topk(torch.mean(x[0],(1,2)),topcount).indices:\n",
    "\t\t\taxs[aidx].imshow(x[0,i])\n",
    "\t\t\taidx+=1\n",
    "\n",
    "plottop(x4c.grad)\n",
    "plottop(x3c.grad)\n",
    "plottop(x2c.grad)\n",
    "plottop(x1c.grad)\n",
    "plottop(x0c.grad)\n",
    "plt.subplots()[1].imshow(xinput.grad[0].permute(1,2,0)*50.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if optimize image\n",
    "with torch.no_grad(): img = torch.zeros((1,3,224,224)).requires_grad_(True)\n",
    "attentionToWhat = torch.zeros(output.shape)\n",
    "attentionToWhat[0,labels.index(b\"ashcan\\n\")] = 1.0\n",
    "\n",
    "for i in range(10):\n",
    "\toutput = resnet(img)\n",
    "\n",
    "\t(torch.square(attentionToWhat-output)).sum(1).backward()\n",
    "\twith torch.no_grad(): img -= img.grad*0.1\n",
    "\timg.grad.zero_()\n",
    "output = resnet(img)\n",
    "plt.imshow(img.detach()[0].permute(1,2,0))\n",
    "for c in output.topk(10).indices[0]:\n",
    "\tprint(labels[c])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef90df",
   "metadata": {},
   "source": [
    "Sep 14, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cf9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try attention on first layers\n",
    "xinput.grad.zero_()\n",
    "a=torch.zeros(x0.shape)\n",
    "a[0,0:64,20:30,20:30]=1\n",
    "(x0*x0/2.0 * a).sum().backward(retain_graph=True)\n",
    "plt.imshow(xinput.grad[0].permute(1,2,0)*50.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
